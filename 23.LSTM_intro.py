# LSTM RNN хороша тем,что решает задачу прошлых слов(запоминание) у нее происходит намного лучше 
# Более понятно будет просто посмотреть на фотографию,но попробую пересказать 

# 1.Забывание ненужного:
# 1.1 Сначалы мы перемножаем старый вектор(h_t-1) на вектор наших новых данных и после этого вычисляем сигмоидальную функцию(Умножить на веса 
# и добавить bias(важность появления каждого слова))
# 1.2 После этого мы вектор C_t-1 попарно умножаем с вектором,который мы вычислили на прошлом шаге(векторы по shape равны между собой)
# По итогу получается,что мы просто получаем те значения,которые нам важны
# 2. Запоминание нового и складывание с 1.
# 2.1 Мы вычисояем абсолютно такую же сигмоидальную функцию -> получаем вероятности 
# 2.2 Мы вычисляем тангенсоеду такиж же данных и получаем значения,которые нам важны
# 2.3 Мы попарно перемножаем 2.1 * 2.2
# 2.4 Мы попарно складываем пункт 1 + 2.3 -> получаем значения которые нам важны + просто значения,которые появились 
# 3. Создаем значения h_t для нашего слоя RNN
# 3.1 Мы вычисояем абсолютно такую же сигмоидальную функцию -> получаем вероятности
# 3.2 Мы вычисляем тангенсоидную функцию для 2(просто всю память) 
# 3.3 Попарно перемножаем тангенсоидную функию(3.2) с сигмоидальной(3.1)
