# 1.Рекомендация обучения : Запускать алгоритм для разных начальных значений весовых коэфициентов. И,затем,отобрать лучший вариант.
# Начальные значения генерируем случайным образом в окрестности нуля,кроме тех,что относятся к bias'ам.

# 2.Рекомендация обучения : Запускаем алгоритм обучения с оптимизацией по Adam или Нестерову(используються в Keras и TFLearn) для 
# ускорения обучения HC

# 3.Рекомендация обучения : Выполнять нормировку входных значений(ВСЕХ) и запоминать нормированные параметры min,max для обучающей выборки 
# Это нужно чтобы мы не попадали в области насыщения функции(не попадали практически на ровную прямую,где ничего не работает)

# 4.Рекомендация обучения : Помещать в обучающую выборку самые разнообразные данные примерно равного количества 

# 5.Рекомендация обучения : Наблюдения на вход сети подавать случайным образом,корректировать веса после серии наблюдений(mini batch)

# 6.Рекомендация обучения : Использовать минимальное необходимое число нейронов в нейронной сети,чтобы избежать переобучение

# 7.Рекомендация обучения : Разбивать все множество наблюдений на три выборки : обучающую,валидации, и тестовую.
# Так или иначе HC подстраивается под наши выборки,поэтому сначала мы просто тренируем данные,потом смотрим когда качество выбоки 
# валидации расходиться с обучающей выборкой и устанавливаем границу переобучения и весовые коэфициенты -> Тестим на тестовой выборке

# Критерии остановки процесса обучения 
#  * Расхождения показателя качества для обучающей выборки и валидации 
#  * От итерации к итерации (по всей эпохе) показатель качества практически не меняется 
#  * Происходит малое изменение весовых коэфициетов 
#  * Достигли максимального числа итераций 
