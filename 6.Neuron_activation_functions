# Обычно в нейросети разные функции активации в скрытом слое и слое,который уже выводит значение 

# Уже знаем три функции 1.Пороговая 2.Гиперболический тангенс(7 слоев - max),Логистическая функция(7 слоев - max)
# Я указал максимальные слои из-за того,что при подсчете градиента эти функции активации принимают дробные значения и с каждым слоем их
# воздействие на градиент будет уменьшаться - и уже на 7-м слое он практически выравняеться с прямой(следует все из формул)

# Обычно в deep learning(обучение с большим количеством слоев) используеться обычная функция (x) = max(x,0) - (не идет выше 0),
# тк там значения просто равны и градиент будет меняться на любых слоях.
# Такая штука называется ---- ReLu ---- незанемимая 

# 8.Рекомендация обучения : при малом числе слоев можно использовать гиперболическую и сигмоидальную функции активации или ReLu, при 
# числе слоев от 8 и более - Relu и ее вариации 

# Но в задачах на регрессию мы используем только 'linear' функцию,тк нам нужна там только прямая 

# Для задач на классификацию(не бинарную) мы используем 'softmax' ---- y_1 = e_v_1 / e_v_1 * e_v_2 * e_v_3 ......
# Оно дает нам возможность получать все ответы в сумме 1 и выводит "вероятность" ,котрую можем потом интерпретировать в цифрах [0.93 , 0.027 ,0.053]
# при том что вывод должен быть [1,0,0]
# При 'softmax' мы как критерий качества используем 'перекрестную энтропию'

# 9.Рекомендация обучения : для задач регрессии у выходных нейронов использовать линейную(linear) функцию активации,для задач 
# классификации не пересекающихся классов - softmax

# Лучшие критерии оценки неронных сетей для разных задач (E)
#
# Распознавание:
#   1.hinge - увеличиваем параметр E при каждом не совподаемом значении(все должно быть максимально точно)
#   2.binary crossentropy - при классификации двух классов 
#   3.categorical crossntropy - при классификации более двух классов 
# Обработка текста:
#   1. logcosh
# Задачи регрессии:
#   1. mean squared error - очень влияют значимые(большие) ошибки(ответ сети - правильный ответ)
#   2. mean absolute error - не сильно влияют значимые ошибки 
#   3. mean absolute percentage error - убирает редкие, но большие ошибки 
#   4. mean squared logarithmic error - какая-то херня,но там все выводится процентами 
#
