# Ковариационный сдвиг - на примере машин легче всего сказать - когда у нас HC обучаеться сначала на черных машинах,а потом на красных,либо же других
# цветов.Смысл в том что в нейросети у нас сначала одно распределение(как пример нормальное),а потом из-за машин совершенно другого цвета у нас распределение 
# идет по кривой,что сильно сказываеться на весах.И,допустим,сначала у нас нейросеть обучилась на черной машине(изменяя при этом весовые коэфициенты)
# и после этого мы сразу даем ей красную машину и весовые коэфициенты опять сильно меняються,что в свою очередь затрудняет обучение либо по времени
# либо по эффекивности + нам нужно ставить маленький шаг сходимости(L),чтобы сеть смогла адаптироваться 

# Распределение постоянно меняеться от слоя к слою

# Batch Normalization - стандартизирует распределение(мат ожидание - 0, дисперсия - 1)

# Рекомендуют использовать Batch Normalization сразу перед функцией активации или сразу после
# 
# По поводу математики легче сказать что оно работает через мат.ожидание,дисперсию и впоследствии смещение(чтобы отличать красное от черного)
# 
# Что дает(возможно даст)?
# 1.Ускорение сходимости модели обучающей выборки 
# 2.Большая независимость обучения каждого слоя нейрона 
# 3.Возможность увеличения шага обучения(L)
# 4.В некоторой степени предотвращает эффект переобучения 
# 5.Меньшая чувствительность к начальной инициализации весовых коэфициентов

# Работает в Deep Learning 

# 1.Рекомендация: Изначально строить нейронные сети без batch normalization( или dropout ) и если наблюдается медленное обучение или эффект 
# переобучения, то можно попробовать добавить batch normalization или dropout,но не оба вместе 

from keras.layers import BatchNormalization
BatchNormalization()        # Ставиться сразу после слоя,к которому он применяется 
